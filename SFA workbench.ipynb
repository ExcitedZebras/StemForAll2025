{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ee1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm  # progress bar for nicer output\n",
    "from time import sleep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  \\\n",
      "5    6 . 5 Magnitude Earthquake Reported | 720 THE ...   \n",
      "7    This Unexpected South Florida City Might Be th...   \n",
      "11   EU - US trade deal expected to confirm duty - ...   \n",
      "12   SkyWest balks at prospect of paying steep tari...   \n",
      "19   Harper says Carney team sought his trade advic...   \n",
      "..                                                 ...   \n",
      "237  As U . S . plans to exit UNESCO , Ohio earthwo...   \n",
      "239  A raw deal but the best they could get with Tr...   \n",
      "240  US Trade Rep Expects No  Enormous Breakthrough...   \n",
      "247  In brief :  The Walking Dead : Daryl Dixon  re...   \n",
      "249  Trump moves to lift visa restrictions for Arge...   \n",
      "\n",
      "                                                   url          seendate  \\\n",
      "5    https://720thevoice.iheart.com/content/2025-07...  20250728T201500Z   \n",
      "7    https://www.miaminewtimes.com/restaurants/this...  20250728T201500Z   \n",
      "11   https://article.wn.com/view/2025/07/28/EUUS_tr...  20250728T201500Z   \n",
      "12   https://www.flightglobal.com/skywest-balks-at-...  20250728T201500Z   \n",
      "19   https://lethbridgeherald.com/news/national-new...  20250728T201500Z   \n",
      "..                                                 ...               ...   \n",
      "237  https://wysu.org/ohio-news/2025-07-28/as-u-s-p...  20250728T200000Z   \n",
      "239  https://lasvegassun.com/news/2025/jul/28/a-raw...  20250728T200000Z   \n",
      "240  https://www.theepochtimes.com/china/us-trade-r...  20250728T200000Z   \n",
      "247  https://www.brookingsradio.com/syndicated-arti...  20250728T200000Z   \n",
      "249  https://www.clickondetroit.com/news/world/2025...  20250728T200000Z   \n",
      "\n",
      "     sourcecountry  \n",
      "5    United States  \n",
      "7    United States  \n",
      "11   United States  \n",
      "12   United States  \n",
      "19   United States  \n",
      "..             ...  \n",
      "237  United States  \n",
      "239  United States  \n",
      "240  United States  \n",
      "247  United States  \n",
      "249  United States  \n",
      "\n",
      "[91 rows x 4 columns]\n",
      "Saved to gdelt_us_articles.csv ✅\n"
     ]
    }
   ],
   "source": [
    "# ## code before hand\n",
    "\n",
    "# url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "# params = {\n",
    "#     \"query\": \"united states\",\n",
    "#     \"mode\": \"artlist\",\n",
    "#     \"maxrecords\": 250,  # slightly higher in case non-US articles are included\n",
    "#     \"format\": \"json\",\n",
    "#     \"sort\": \"datedesc\",\n",
    "#     \"sourcecountry\": \"US\"\n",
    "# }\n",
    "\n",
    "# response = requests.get(url, params=params)\n",
    "\n",
    "# if response.status_code != 200:\n",
    "#     print(\"Error:\", response.status_code)\n",
    "#     print(response.text)\n",
    "# else:\n",
    "#     data = response.json()\n",
    "#     articles = data.get(\"articles\", [])\n",
    "    \n",
    "#     # Convert to DataFrame\n",
    "#     df = pd.DataFrame(articles)\n",
    "\n",
    "#     # Filter only rows where sourcecountry is \"United States\"\n",
    "#     df_us = df[df[\"sourcecountry\"] == \"United States\"].head(100)\n",
    "\n",
    "#     # Show selected columns\n",
    "#     print(df_us[[\"title\", \"url\", \"seendate\", \"sourcecountry\"]])\n",
    "\n",
    "#     # Save to CSV\n",
    "#     df_us.to_csv(\"gdelt_us_articles.csv\", index=False)\n",
    "#     print(\"Saved to gdelt_us_articles.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d84d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## appears to be a test for opening the url of articles\n",
    "    \n",
    "# url = \"https://www.sbsun.com/2025/07/28/trump-gaza-food/\"\n",
    "# article = Article(url)\n",
    "# article.download()\n",
    "# article.parse()\n",
    "\n",
    "# print(article.title)\n",
    "# print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting article content for each URL... This may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:01<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved articles with content to gdelt_us_articles_ezra.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: Query GDELT Doc API for recent US articles\n",
    "# url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "# params = {\n",
    "#     \"query\": \"trump AND harris AND sourcelang:english AND theme:ELECTION AND sourcecountry:US\",\n",
    "#     \"mode\": \"artlist\",\n",
    "#     \"maxrecords\": 250,\n",
    "#     \"format\": \"json\",\n",
    "#     \"sort\": \"datedesc\",\n",
    "#     \"STARTDATETIME\" : 20241103000000,\n",
    "#     \"ENDDATETIME\" : 20241104000000\n",
    "# }\n",
    "\n",
    "# response = requests.get(url, params=params)\n",
    "# if response.status_code != 200:\n",
    "#     print(\"Error:\", response.status_code)\n",
    "#     print(response.text)\n",
    "#     exit()\n",
    "\n",
    "# data = response.json()\n",
    "# articles = data.get(\"articles\", [])\n",
    "\n",
    "# # Step 2: Convert to DataFrame\n",
    "# df = pd.DataFrame(articles)\n",
    "\n",
    "# # Filter only \"United States\" sourcecountry and limit to 100 articles\n",
    "# df_us = df#[df[\"sourcecountry\"] == \"United States\"]#.head(100)\n",
    "\n",
    "# # # Step 3: Extract article content using newspaper3k\n",
    "# # def extract_content(article_url):\n",
    "# #     try:\n",
    "# #         article = Article(article_url)\n",
    "# #         article.download()\n",
    "# #         article.parse()\n",
    "# #         return article.text\n",
    "# #     except Exception as e:\n",
    "# #         return None  # or you can return str(e) to debug failures\n",
    "\n",
    "# # print(\"Extracting article content for each URL... This may take some time.\")\n",
    "# # df_us[\"content\"] = [extract_content(url) for url in tqdm(df_us[\"url\"])]\n",
    "\n",
    "# # # Optional: Drop articles with no content\n",
    "# # df_us = df_us.dropna(subset=[\"content\"])\n",
    "\n",
    "\n",
    "# # Step 4: Save to CSV\n",
    "# df_us.to_csv(\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\gdelt_us_articles_sorted.csv\", index=True)#, quotechar=\"\"\")\n",
    "# print(\"Saved articles with content to gdelt_us_articles_ezra.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ac786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders 'us_articles\\2012 election cycle' already exist.\n",
      "Folders 'us_articles\\2012 election cycle\\month 4' already exist.\n",
      "Error: 200\n",
      "Invalid query start date.\n",
      "\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Woochoel Shin\\Pictures\\Ezras stuff temporary\\coding\\Stemforall\\.venv\\Lib\\site-packages\\requests\\models.py:976\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError:\u001b[39m\u001b[33m\"\u001b[39m, response.status_code)\n\u001b[32m     97\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m data = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m articles = data.get(\u001b[33m\"\u001b[39m\u001b[33marticles\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Woochoel Shin\\Pictures\\Ezras stuff temporary\\coding\\Stemforall\\.venv\\Lib\\site-packages\\requests\\models.py:980\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "### scraper for a year\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep \n",
    "import os\n",
    "\n",
    "###\n",
    "# ANNOUNCEMENT: when scraping for other elections, make sure to change the year and the querry to the candidates for that election cycle\n",
    "###\n",
    "\n",
    "\n",
    "# Query GDELT Doc API for recent US articles\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "days_in_month = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 4] # number of days in each month to make the nested for loops easier to implement\n",
    "# november only has ten days because we don't need to get articles from past election day\n",
    "\n",
    "year = 2012 ### CHANGE THIS DEPENDING ON THE ELECTION CYCLE\n",
    "candidates = \"obama AND romney\" ### CHANGE THIS DEPENDING ON THE ELECTION CYCLE\n",
    "\n",
    "# creating folders for the year if it already doesn't exist\n",
    "path = f\"us_articles\\\\{year} election cycle\"\n",
    "try:\n",
    "    os.makedirs(path)\n",
    "    print(f\"Folders '{path}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folders '{path}' already exist.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating folders: {e}\")\n",
    "\n",
    "### loop through jan-nov\n",
    "for month in range(4, 12): # exclude december bc the election results are out by then\n",
    "    \n",
    "    # creating folders for each month if it doesnt exist already\n",
    "    path = f\"us_articles\\\\{year} election cycle\\\\month {month}\"\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "        print(f\"Folders '{path}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Folders '{path}' already exist.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating folders: {e}\")\n",
    "\n",
    "    ### each day gets its own dataset\n",
    "    for day in range(1, days_in_month[month-1]+1): #plus one error on the months bc the list indexes starting with 0, but january is represented by 1 and plus one error on the number of days in the month because range() is non inclusive at the end\n",
    "        \n",
    "        # code to reset the DFs and adding a header only\n",
    "        response = requests.get(url, params={\"query\":\"icecream\",\"mode\":\"artlist\",\"maxrecords\": 1,\"format\": \"json\"})\n",
    "        \n",
    "        sleeptime = 45 # sometimes the api softlocks us from making requests, so we have to wait it out\n",
    "        while response.status_code == 429:\n",
    "            print(f\"sleeping for {sleeptime} seconds, wait 5 seconds my ass\")\n",
    "            sleep(sleeptime)\n",
    "            response = requests.get(url, params={\"query\":\"icecream\",\"mode\":\"artlist\",\"maxrecords\": 1,\"format\": \"json\"})\n",
    "            sleeptime += 5\n",
    "\n",
    "        data = response.json()\n",
    "        article = data.get(\"articles\", [])\n",
    "        me = pd.DataFrame(article)\n",
    "        me=me.drop(0)\n",
    "        me.to_csv(f\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\us_articles\\\\{year} election cycle\\\\month {month}\\\\{year}-{month}-{day}_gdelt_articles.csv\", index=False)#, quotechar=\"\"\")\n",
    "        \n",
    "        # changes start and end to be modified by fstrings, not math. \n",
    "        # this means you can now start the scraping process at any date by adjusting the range of the nested for loops\n",
    "        start = int(f\"{year}{month:02d}{day:02d}000000\")\n",
    "        end = int(f\"{year}{month:02d}{day:02d}003000\")\n",
    "\n",
    "        ### scrape every hour within a day, then add it to the \"day\" csv file\n",
    "        for hour in range(0,24): #idr why this starts at 0 while the other two loops start at 1\n",
    "            \n",
    "            # reaching out to the api\n",
    "            params = {\n",
    "                \"query\": f\"{candidates} AND sourcelang:english AND theme:ELECTION AND sourcecountry:US\",\n",
    "                \"mode\": \"artlist\",\n",
    "                \"maxrecords\": 250,\n",
    "                \"format\": \"json\",\n",
    "                \"sort\": \"datedesc\",\n",
    "                \"STARTDATETIME\" : start, \n",
    "                \"ENDDATETIME\" : end #for some reason, gdelt gives us articles 45 minutes past the endtime i give it\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            # waiting out the softlock, previously thought to be 45 sec, but when we increased the wait time to 45, we still ended up waiting for a total of 520 seconds. \n",
    "            # new hypothesis is now 500 seconds\n",
    "            sleeptime = 500\n",
    "            while response.status_code == 429:\n",
    "                print(f\"sleeping for {sleeptime} seconds, wait 5 seconds my ass\")\n",
    "                sleep(sleeptime)\n",
    "                response = requests.get(url, params=params)\n",
    "                sleeptime += 5\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(\"Error:\", response.status_code)\n",
    "                print(response.text)\n",
    "                exit()\n",
    "                \n",
    "            # print(\"Error:\", response.status_code)\n",
    "            # print(response.text)\n",
    "\n",
    "            data = response.json()\n",
    "            articles = data.get(\"articles\", [])\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(articles)\n",
    "\n",
    "            # append this hour's worth of articles to the day csv\n",
    "            df.to_csv(f\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\us_articles\\\\{year} election cycle\\\\month {month}\\\\{year}-{month}-{day}_gdelt_articles.csv\", mode='a', header=False, index=False)#, quotechar=\"\"\")\n",
    "            \n",
    "            # print the number of articles that we downloaded from that hour\n",
    "            # print(f\"{hour}. Saved {str(len(df))} articles from time {start} to {end}\")\n",
    "\n",
    "            # move up by an hour. the time resets when start,end is set\n",
    "            start += 10000\n",
    "            end += 10000\n",
    "\n",
    "        # stupid method to index the entire thing. reads the entire csv, then it replaces that same csv but now with indexes\n",
    "        df_day = pd.read_csv(f\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\us_articles\\\\{year} election cycle\\\\month {month}\\\\{year}-{month}-{day}_gdelt_articles.csv\")\n",
    "        df_day.to_csv(f\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\us_articles\\\\{year} election cycle\\\\month {month}\\\\{year}-{month}-{day}_gdelt_articles.csv\", header=True, index=True)\n",
    "\n",
    "        # prints number of articles downloaded on each day\n",
    "        print(f\"day {day} done: {len(df_day)} articles downloaded\")\n",
    "\n",
    "    # keep track of what months are done\n",
    "    print(f\"month {month} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84f881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting article content for each URL... This may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:12<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "### adding sentiment analysis\n",
    "import requests\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm  # progress bar for nicer output\n",
    "from time import sleep \n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "days_in_month = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 4] # number of days in each month to make the nested for loops easier to implement\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\"\n",
    "year = 2024\n",
    "\n",
    "def extract_content(article_url):\n",
    "    try:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return None  # or you can return str(e) to debug failures\n",
    "\n",
    "for month in range(1, 2): # exclude december bc the election results are out by then\n",
    "\n",
    "    ### each day gets its own dataset\n",
    "    for day in range(1, 2):#days_in_month[month-1]+1): #plus one error on the months bc the list indexes starting with 0, but january is represented by 1 and plus one error on the number of days in the month because range() is non inclusive at the end\n",
    "        \n",
    "        df_day = pd.read_csv('z_test_gdelt_us_articles.csv')#f\"{file_path}\\\\us_articles\\\\{year} election cycle\\\\month {month}\\\\{year}-{month}-{day}_gdelt_articles.csv\")\n",
    "\n",
    "        # df_day['content'] = Article(df_day['url'].str)\n",
    "\n",
    "        print(\"Extracting article content for each URL... This may take some time.\")\n",
    "        df_day[\"content\"] = [extract_content(url) for url in tqdm(df_day[\"url\"])]\n",
    "\n",
    "        # Optional: Drop articles with no content\n",
    "        # df_day = df_day.dropna(subset=[\"content\"])\n",
    "\n",
    "df_day.to_csv(\"testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b224310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 200\n",
      "{}\n",
      "hour 0. Saved 0 articles from time 20240101000000 to 20240101003000\n",
      "Error: 200\n",
      "{}\n",
      "hour 1. Saved 0 articles from time 20240101010000 to 20240101013000\n",
      "Error: 200\n",
      "{}\n",
      "hour 2. Saved 0 articles from time 20240101020000 to 20240101023000\n",
      "Error: 200\n",
      "{\"articles\": [ { \"url\": \"https://www.fitsnews.com/2023/12/31/palmetto-political-stock-index-previewing-a-pivotal-election-year/\", \"url_mobile\": \"\", \"title\": \"Palmetto Political Stock Index : Previewing A Pivotal Election Year\", \"seendate\": \"20240101T030000Z\", \"socialimage\": \"https://www.fitsnews.com/wp-content/uploads/2023/12/gettyimages-1860116039-1024x768.jpg\", \"domain\": \"fitsnews.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n",
      "hour 3. Saved 1 articles from time 20240101030000 to 20240101033000\n",
      "Error: 200\n",
      "{}\n",
      "hour 4. Saved 0 articles from time 20240101040000 to 20240101043000\n",
      "Error: 200\n",
      "{}\n",
      "hour 5. Saved 0 articles from time 20240101050000 to 20240101053000\n",
      "Error: 200\n",
      "{\"articles\": [ { \"url\": \"https://legalinsurrection.com/2023/12/legal-insurrection-authors-predictions-for-2024/\", \"url_mobile\": \"https://legalinsurrection.com/2023/12/legal-insurrection-authors-predictions-for-2024/amp/\", \"title\": \"Legal Insurrection Author Predictions For 2024\", \"seendate\": \"20240101T060000Z\", \"socialimage\": \"https://legalinsurrection.com/wp-content/uploads/2023/12/Happy-New-Year-via-Australia.png\", \"domain\": \"legalinsurrection.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n",
      "hour 6. Saved 1 articles from time 20240101060000 to 20240101063000\n",
      "Error: 200\n",
      "{}\n",
      "hour 7. Saved 0 articles from time 20240101070000 to 20240101073000\n",
      "Error: 200\n",
      "{}\n",
      "hour 8. Saved 0 articles from time 20240101080000 to 20240101083000\n",
      "Error: 200\n",
      "{}\n",
      "hour 9. Saved 0 articles from time 20240101090000 to 20240101093000\n",
      "Error: 200\n",
      "{}\n",
      "hour 10. Saved 0 articles from time 20240101100000 to 20240101103000\n",
      "Error: 200\n",
      "{\"articles\": [ { \"url\": \"https://www.windermeresun.com/2023/12/31/what-to-expect-in-2024/\", \"url_mobile\": \"\", \"title\": \"What To Expect In 2024 - Windermere Sun - For Healthier / Happier / More Sustainable Living\", \"seendate\": \"20240101T110000Z\", \"socialimage\": \"https://www.windermeresun.com/wp-content/uploads/2023/12/crystal-ball-pexels-photo-902155-final.jpg\", \"domain\": \"windermeresun.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" },{ \"url\": \"https://www.bostonherald.com/2024/01/01/curley-banning-trump-from-ballots-a-sure-way-to-get-him-reelected/\", \"url_mobile\": \"https://www.bostonherald.com/2024/01/01/curley-banning-trump-from-ballots-a-sure-way-to-get-him-reelected/amp/\", \"title\": \"Curley : Banning Trump from ballots a sure way to get him reelected\", \"seendate\": \"20240101T110000Z\", \"socialimage\": \"https://www.bostonherald.com/wp-content/uploads/2023/12/GettyImages-1868621418.jpg\", \"domain\": \"bostonherald.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" },{ \"url\": \"https://news.yahoo.com/fraying-coalition-black-hispanic-young-100855764.html\", \"url_mobile\": \"\", \"title\": \"A fraying coalition : Black , Hispanic , young voters abandon Biden as election year begins\", \"seendate\": \"20240101T110000Z\", \"socialimage\": \"https://s.yimg.com/ny/api/res/1.2/tqlIndueGMJu0h4pIrJ6AA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/usa_today_news_641/9dce26a480c1bb491fb22842f1514953\", \"domain\": \"news.yahoo.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" },{ \"url\": \"https://www.arkansasonline.com/news/2024/jan/01/the-horrid-no-good-year/\", \"url_mobile\": \"\", \"title\": \"The horrid , no - good year | The Arkansas Democrat - Gazette - Arkansa Best News Source\", \"seendate\": \"20240101T110000Z\", \"socialimage\": \"\", \"domain\": \"arkansasonline.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n",
      "hour 11. Saved 4 articles from time 20240101110000 to 20240101113000\n",
      "Error: 200\n",
      "{\"articles\": [ { \"url\": \"https://nypost.com/2024/01/01/opinion/to-win-in-2024-biden-and-trump-must-fight-for-our-hearts/\", \"url_mobile\": \"https://nypost.com/2024/01/01/opinion/to-win-in-2024-biden-and-trump-must-fight-for-our-hearts/amp/\", \"title\": \"To win in 2024 , Biden and Trump must fight for our hearts\", \"seendate\": \"20240101T124500Z\", \"socialimage\": \"https://nypost.com/wp-content/uploads/sites/2/2023/12/2020-case-western-university-cleveland-73534721-e1704063366118.jpg?quality=75&strip=all\", \"domain\": \"nypost.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" },{ \"url\": \"https://patriotpost.us/opinion/103217-trump-biden-and-a-fight-for-the-heart-2024-01-01\", \"url_mobile\": \"\", \"title\": \"Daniel McCarthy : Trump , Biden and a Fight for the Heart\", \"seendate\": \"20240101T123000Z\", \"socialimage\": \"https://img.patriotpost.us/01GR4QNK1K7PV7B16TYRNJ89RV.jpeg\", \"domain\": \"patriotpost.us\", \"language\": \"English\", \"sourcecountry\": \"United States\" },{ \"url\": \"https://patriotpost.us/opinion/103219-the-top-10-non-stories-of-2023-2023-12-30\", \"url_mobile\": \"\", \"title\": \"The Washington Stand : The Top 10 Non - Stories of 2023\", \"seendate\": \"20240101T123000Z\", \"socialimage\": \"https://img.patriotpost.us/01GR4QNK1K7PV7B16TYRNJ89RV.jpeg\", \"domain\": \"patriotpost.us\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n",
      "hour 12. Saved 3 articles from time 20240101120000 to 20240101123000\n",
      "Error: 200\n",
      "{\"articles\": [ { \"url\": \"https://internationalviewpoint.org/spip.php?article8367\", \"url_mobile\": \"\", \"title\": \"Election 2024 Deform & Dysfunction - International Viewpoint - online socialist magazine\", \"seendate\": \"20240101T131500Z\", \"socialimage\": \"\", \"domain\": \"internationalviewpoint.org\", \"language\": \"English\", \"sourcecountry\": \"United States\" },{ \"url\": \"https://prescottenews.com/index.php/2024/01/01/trump-is-blocked-from-the-gop-primary-ballot-in-two-states-can-he-still-run-for-president-associated-press/\", \"url_mobile\": \"\", \"title\": \"Trump is blocked from the GOP primary ballot in two states . Can he still run for president ? – Associated Press\", \"seendate\": \"20240101T131500Z\", \"socialimage\": \"\", \"domain\": \"prescottenews.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" },{ \"url\": \"https://news.yahoo.com/election-unlike-ve-lived-democrats-110000779.html\", \"url_mobile\": \"\", \"title\": \"It Will Be an Election Unlike Any Weve Lived Through . Are the Democrats Prepared ? \", \"seendate\": \"20240101T131500Z\", \"socialimage\": \"https://media.zenfs.com/en/the_new_republic_521/2a1e17556d9a6eb947f28704b086e480\", \"domain\": \"news.yahoo.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n",
      "hour 13. Saved 3 articles from time 20240101130000 to 20240101133000\n",
      "Error: 200\n",
      "{}\n",
      "hour 14. Saved 0 articles from time 20240101140000 to 20240101143000\n",
      "Error: 200\n",
      "{}\n",
      "hour 15. Saved 0 articles from time 20240101150000 to 20240101153000\n",
      "Error: 200\n",
      "{\"articles\": [ { \"url\": \"https://www.wnd.com/2024/01/trump-dems-hold-2024-free-replace-scoundrel-biden/\", \"url_mobile\": \"\", \"title\": \"Trump : Dems to hold 2024  free - for - all  to replace  scoundrel  Biden\", \"seendate\": \"20240101T161500Z\", \"socialimage\": \"https://www.wnd.com/wp-content/uploads/2023/05/joe-biden-pointing-shadows-angry-big-brother-bw.jpg\", \"domain\": \"wnd.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n",
      "hour 16. Saved 1 articles from time 20240101160000 to 20240101163000\n",
      "Error: 200\n",
      "{}\n",
      "hour 17. Saved 0 articles from time 20240101170000 to 20240101173000\n",
      "Error: 200\n",
      "{}\n",
      "hour 18. Saved 0 articles from time 20240101180000 to 20240101183000\n",
      "Error: 200\n",
      "{}\n",
      "hour 19. Saved 0 articles from time 20240101190000 to 20240101193000\n",
      "Error: 200\n",
      "{\"articles\": [ { \"url\": \"https://www.wnd.com/2024/01/behind-bidens-sliding-poll-numbers/\", \"url_mobile\": \"\", \"title\": \"What behind Biden sliding poll numbers ? \", \"seendate\": \"20240101T204500Z\", \"socialimage\": \"https://www.wnd.com/wp-content/uploads/2023/05/joe-biden-smiling-podium.jpg\", \"domain\": \"wnd.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n",
      "hour 20. Saved 1 articles from time 20240101200000 to 20240101203000\n",
      "Error: 200\n",
      "{}\n",
      "hour 21. Saved 0 articles from time 20240101210000 to 20240101213000\n",
      "Error: 200\n",
      "{}\n",
      "hour 22. Saved 0 articles from time 20240101220000 to 20240101223000\n",
      "Error: 200\n",
      "{}\n",
      "hour 23. Saved 0 articles from time 20240101230000 to 20240101233000\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "### this is the code for scraping a single day\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep \n",
    "\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "# code to reset the DF\n",
    "response = requests.get(url, params={\"query\": \"icecream\",\"mode\": \"artlist\",\"maxrecords\": 1,\"format\": \"json\"})\n",
    "data = response.json()\n",
    "article = data.get(\"articles\", [])\n",
    "me = pd.DataFrame(article)\n",
    "me=me.drop(0)\n",
    "me.to_csv(\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\z_test_gdelt_us_articles.csv\", index=False)\n",
    "\n",
    "# me = pd.DataFrame()\n",
    "# me.to_csv(\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\test_gdelt_us_articles.csv\", index=False)\n",
    "\n",
    "start = 20240101000000 #20241103000000\n",
    "end = 20240101003000 #20241103003000\n",
    "count = 0\n",
    "\n",
    "for hour in range(0,24):\n",
    "    params = {\n",
    "        \"query\": \"trump AND harris AND sourcelang:english AND theme:ELECTION AND sourcecountry:US\",\n",
    "        \"mode\": \"artlist\",\n",
    "        \"maxrecords\": 250,\n",
    "        \"format\": \"json\",\n",
    "        \"sort\": \"datedesc\",\n",
    "        \"STARTDATETIME\" : start, \n",
    "        \"ENDDATETIME\" : end #for some reason, gdelt gives us articles 45 minutes past the endtime i give it\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error:\", response.status_code)\n",
    "        print(response.text)\n",
    "        exit()\n",
    "\n",
    "    # print(\"Error:\", response.status_code)\n",
    "    # print(response.text)\n",
    "\n",
    "    data = response.json()\n",
    "    articles = data.get(\"articles\", [])\n",
    "\n",
    "    # Step 2: Convert to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # print(df)\n",
    "\n",
    "    # Step 4: Save to CSV\n",
    "    df.to_csv(f\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\z_test_gdelt_us_articles.csv\", mode='a', header=False, index=False)#, quotechar=\"\"\")\n",
    "    \n",
    "    # print the number of articles that we downloaded from that hour\n",
    "    print(f\"hour {count}. Saved {str(len(df))} articles from time {start} to {end}\")\n",
    "\n",
    "\n",
    "    start += 10000\n",
    "    end += 10000\n",
    "    count+=1\n",
    "\n",
    "# stupid method to index the entire thing\n",
    "bruh = pd.read_csv(\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\z_test_gdelt_us_articles.csv\")\n",
    "bruh.to_csv(\"C:\\\\Users\\\\Woochoel Shin\\\\Pictures\\\\Ezras stuff temporary\\\\coding\\\\Stemforall\\\\z_test_gdelt_us_articles.csv\", header=True, index=True)\n",
    "print(len(bruh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of those articles actually exist\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "params = {\n",
    "    \"query\": \"trump AND harris\",\n",
    "    \"mode\": \"TimelineVolRaw\",\n",
    "    \"maxrecords\": 250,\n",
    "    \"format\": \"json\",\n",
    "    \"sort\": \"datedesc\",\n",
    "    \"sourcecountry\": \"United States\",\n",
    "    \"sourcelang\" : \"english\",\n",
    "    \"theme\" : \"ELECTION\",\n",
    "    \"STARTDATETIME\" : 20240904000000,\n",
    "    \"ENDDATETIME\" : 20240904010000#20241104000000\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "print(response.type())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
